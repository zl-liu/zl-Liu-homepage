<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Zhengliang Liu - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Zhengliang Liu">
<meta property="og:title" content="Zhengliang Liu">


  <link rel="canonical" href="http://localhost:4001/">
  <meta property="og:url" content="http://localhost:4001/">



  <meta property="og:description" content="A PH.D. student from University of Georgia">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
<link rel="manifest" href="/images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  <script src="http://127.0.0.1:35729/livereload.js"></script></head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="/">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/">About</a></li>
          
            <li class="masthead__menu-item"><a href="/news/">News</a></li>
          
            <li class="masthead__menu-item"><a href="/publications/">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/teaching/">Experience</a></li>
          
            <li class="masthead__menu-item"><a href="/activities/">Activities</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="/images/avatar.png" class="author__avatar" alt="Zhengliang Liu">
  </div>

  <div class="author__content">
    <h3 class="author__name">Zhengliang Liu</h3>
    <p class="author__bio">University of Georgia</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">A PH.D. student from University of Georgia</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Athens, GA</li>
      
      
      
      
        <li><a href="mailto:zl18864@uga.edu"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
        <li><a href="https://www.linkedin.com/in/zhengliang-liu"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
      
        <li><a href="https://github.com/zl-liu"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=p8tAM0AAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0003-2567-627X"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:zl18864@uga.edu"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
        <a href="https://www.linkedin.com/in/zhengliang-liu"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
        <a href="https://github.com/zl-liu"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=p8tAM0AAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
        <a href="https://orcid.org/0000-0003-2567-627X"><i class="ai ai-orcid-square ai-fw"></i></a>
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            <p><span class="anchor" id="about-me"></span></p>

<h1 id="short-bio">Short Bio</h1>

<p>Zhengliang Liu is a Ph.D. student at the <a href="https://www.uga.edu/">University of Georgia</a>, advised by <a href="https://cobweb.cs.uga.edu/~tliu/">Prof. Tianming Liu</a>. His research interests include Large Language Models, Medical Imaging, AI for Healthcare, and Visualization. He has published multiple papers in top AI and medical imaging journals and conferences. He received his Bachelor‚Äôs degree from University of Wisconsin in 2018.</p>

<!-- Lin Zhao is an Assistant Professor at the [Department of Biomedical Engineering](https://biomedical.njit.edu/), [New Jersey Institute of Technology (NJIT)](https://www.njit.edu/), and the Director of the Machine Intelligence in Medicine and Imaging Lab (MI<sup>2</sup> Lab). He was a Senior Research Scientist at United Imaging Intelligence (UII) America from 2023 to 2025. He received his PhD degree in Computer Science from University of Georgia in 2023 under the supervision of [Prof. Tianming Liu](https://cobweb.cs.uga.edu/~tliu/). During his PhD study, he also interned at Alibaba DAMO Academy in 2022 and United Imaging Intelligence in 2021. He received his Bachelor degree from Northwestern Polytechnical University in 2017. Currently. His recent research interests include Brain-inspired AI, Large Foundation Models, Vision-Language Modeling and their applications in medical imaging. He has published over 50 papers <a href='https://scholar.google.com/citations?user=RxG1Wj8AAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Flin-zhao-research%2Flin-zhao-research.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a> at the top AI and medical imaging jounrals and conferences.  -->

<h1 id="research-interests">Research Interests</h1>

<p>My research interests lie at the intersection of Artificial Intelligence, Large Language Models (LLMs), and Healthcare. I am particularly passionate about Multi-modal Learning, which seeks to synergize diverse data modalities‚Äîsuch as clinical text, medical imaging, and structured electronic health records‚Äîto construct holistic and context-aware AI systems. In parallel, I aim to leverage cutting-edge Generative AI techniques to tackle real-world biomedical and clinical challenges, such as model reliability, reasoning under uncertainty, and clinical workflow automation. Another key area of interest is AI for Social Good, particularly in utilizing AI agents to democratize access to high-quality education and healthcare resources. I focus on developing robust, human-centered AI solutions that not only advance technical frontiers but also drive tangible positive impacts on patient outcomes and societal well-being.</p>

<h1 id="-education">üéì Education</h1>
<ul>
  <li><strong>University of Georgia</strong>: Ph.D. in Computer Science, GPA: 4.0/4.0, <em>Jun. 2026 (expected graduation)</em></li>
  <li><strong>Washington University in St. Louis</strong>: Master of Science in Computer Science, GPA: 3.9/4.0, <em>2021</em></li>
  <li><strong>University of Wisconsin, Madison</strong>: Bachelor of Arts in Computer Science, GPA: 3.904/4.0, <em>2018</em></li>
</ul>

<!-- # üéì Prospective Student
<span style="color: red;"> **I am actively seeking self-motivated PhD students and interns.**</span> If you are interested in working with me, please check the details below and feel free to email me at [lin.zhao.1@njit.edu](mailto:lin.zhao.1@njit.edu).

|**PhD Openings**| I am actively looking for multiple highly motivated PhD students starting from Spring 2026. If you're interested in collaborating with me for your doctoral studies, please reach out to me early and submit your application to the [NJIT/Rutgers Joint Ph.D. Program in Biomedical Engineering](https://biomedical.njit.edu/academics/graduate/doctoral.php) and be sure to include my name in your application materials. |

|**Intern Openings**|I am also looking for self-motivated remote and in-person intern students starting at any time. My past interns have secured first-author publications in top jounrals and conferences such as TNNLS/TMI/MedIA/MICCAI. I would prefer that you are interested in working with me as a PhD student in the long run.|

**Email Format**: Please use the subject line: "Interest in {Position, e.g., Ph.D.} ‚Äì {Start Term, e.g., Spring 2026} ‚Äì {Your Name}" and attach your CV. Thanks!

**Note**: I carefully read every email I receive. However, due to the high volume of inquiries, I may not be able to respond to each one individually. Thank you for your understanding. -->

<h1 id="-honors-and-awards">ü•á Honors and Awards</h1>
<ul>
  <li><em>2016-2018</em> <strong>Dean‚Äôs List</strong>, University of Wisconsin, Madison</li>
  <li><em>2018</em> <strong>Graduated with Distinction</strong>, University of Wisconsin, Madison</li>
  <li><em>2023</em> <strong>Student Travel Grant</strong>, MICCAI</li>
</ul>

<h1 id="-news-more">üî• News <a href="\news\">[more‚Ä¶]</a></h1>
<ul>
  <li><em>2025.12</em>: üéâ  Received an <strong>NVIDIA Academic Grant</strong> to support our research on federated learning for foundation models in medical imaging.</li>
  <li><em>2025.09</em>: üéâ One paper on correspondence matching in coronary angiography is accepted by IEEE Transactions on Medical Imaging (TMI).</li>
  <li><em>2025.09</em>: üéâ MI<sup>2</sup> Lab Launched at New Jersey Institute of Technology.</li>
  <li><em>2025.05</em>: üéâ One paper on real-time ultrasound image segmentation using foundation models is early accepted by MICCAI 2025.</li>
  <li><em>2025.04</em>: üéâ One review paper on large language model for radiology is accepted by Meta-Radiology.</li>
  <li><em>2025.04</em>: üéâ One review paper on fMRI-based brain function mapping is accepted by Psychoradiology.</li>
  <li><em>2025.04</em>: üéâ One paper on few-shot medical image segmentation is accepted by IEEE Transactions on Neural Networks and Learning Systems (TNNLS).</li>
  <li><em>2025.04</em>: I will be joining the <a href="https://biomedical.njit.edu/">Department of Biomedical Engineering</a> at <a href="https://www.njit.edu/">New Jersey Institute of Technology</a> as a Tenure-Track Assistant Professor from Fall 2025.</li>
</ul>

<h1 id="-recent-publications-more">üìù Recent Publications <a href="\publications\">[more‚Ä¶]</a></h1>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">AIED 2023</div><img src="/images/selected papers/Context Matters_ A Strategy to Pre_train.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://link.springer.com/chapter/10.1007/978-3-031-36336-8_103">Context matters: A strategy to pre-train language model for science education</a> <br />
<strong>Z. Liu</strong>, X. He, L. Liu, T. Liu, and X. Zhai</p>

    <p>This study proposes a domain-specific pre-training strategy that significantly improves the automatic scoring of student science responses by continually training BERT models on specialized educational corpora (such as student answers and journal articles) to better capture the unique linguistic patterns of student scientific argumentation.</p>

    <p><a href="https://arxiv.org/abs/2301.12031"><img src="https://img.shields.io/badge/arxiv-2301.12031-white?style=flat&amp;logo=arxiv" alt="Static Badge" /></a></p>

  </div>
</div>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">MICCAI 2025</div><img src='/images/arxiv/2503.24368.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Adapting Vision Foundation Models for Real-time Ultrasound Image Segmentation](https://arxiv.org/pdf/2503.24368) \\
Xiaoran Zhang, Eric Z. Chen, **Lin Zhao**, Xiao Chen, Yikang Liu, Boris Maihe, James S Duncan, Terrence Chen, and Shanhui Sun

[![Static Badge](https://img.shields.io/badge/arxiv-2503.24368-white?style=flat&logo=arxiv)](https://arxiv.org/pdf/2503.24368)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TNNLS</div><img src='/images/arxiv/2408.08813.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Retrieval-Augmented Few-Shot Medical Image Segmentation with Foundation Models](https://arxiv.org/pdf/2408.08813) \\
**Lin Zhao**, Xiao Chen, Eric Z. Chen, Yikang Liu, Terrence Chen, and Shanhui Sun

[![Static Badge](https://img.shields.io/badge/arxiv-2408.08813-white?style=flat&logo=arxiv)](https://arxiv.org/pdf/2408.08813)

</div>
</div> -->

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">MLMI 2023</div><img src="/images/selected papers/Tailoring Large Language Models to Radiology_ A Preliminary Approach to LLM Adaptation for a Highly Specialized Domain.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://link.springer.com/chapter/10.1007/978-3-031-45673-2_46">Tailoring large language models to radiology: A preliminary approach to llm adaptation for a highly specialized domain</a> <br />
<strong>Z. Liu</strong>, A. Zhong, Y. Li, L. Yang, C. Ju, Z. Wu, C. Ma, P. Shu, C. Chen, S. Kim, H. Dai, L. Zhao, D. Zhu, J. Liu, W. Liu, D. Shen, Q. Li, T. Liu, and X. Li</p>

    <p>This preliminary study demonstrates the effectiveness of instruction tuning on radiological data to create a privacy-compliant, domain-specific large language model that outperforms general-purpose models (such as StableLM and LLaMA) in specialized tasks like radiological diagnosis and report generation.</p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">IJCAI 2022</div><img src="/images/selected papers/AgriBERT_ Knowledge_Infused Agricultural Language Models for Matching Food and Nutrition.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://www.researchgate.net/profile/Amulya-Yadav-2/publication/362052926_Forecasting_the_Number_of_Tenants_At-Risk_of_Formal_Eviction_A_Machine_Learning_Approach_to_Inform_Public_Policy/links/642eef0320f25554da139319/Forecasting-the-Number-of-Tenants-At-Risk-of-Formal-Eviction-A-Machine-Learning-Approach-to-Inform-Public-Policy.pdf">AgriBERT: Knowledge-Infused Agricultural Language Models for Matching Food and Nutrition</a> <br />
S. Rezayi *, <strong>Z. Liu</strong> *, Z. Wu, C. Dhakal, B. Ge, C. Zhen, T. Liu, and S. Li
<span style="background-color: #FFF9E6; font-style: italic; border-radius: 2px; font-size: 0.85em; color: #666;">
    (* equal contribution)
</span></p>

    <p>This research presents AgriBERT, a specialized language model pre-trained on agricultural text and enhanced with knowledge infusion from food ontologies, designed to automate and significantly improve the accuracy of mapping unstructured food descriptions to standard nutritional databases.</p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">BIBM 2023</div><img src="/images/selected papers/Coarse_to_fine Knowledge Graph Domain Adaptation based on.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://ieeexplore.ieee.org/abstract/document/10385649">Coarse-to-fine knowledge graph domain adaptation based on distantly-supervised iterative training</a> <br />
W. Liao*, <strong>Z. Liu</strong> *, Y. Zhang, X. Huang, F. Qi, S. Ding, H. Ren, Z. Wu, H. Dai, S. Li, L. Wu, N. Liu, Q. Li, T. Liu, X. Li, and H. Cai
<span style="background-color: #FFF9E6; font-style: italic; border-radius: 2px; font-size: 0.85em; color: #666;">
    (* equal contribution)
</span></p>

    <p>This paper proposes a coarse-to-fine domain adaptation framework that leverages distant supervision and an iterative training strategy to efficiently construct specialized knowledge graphs (such as for oncology) from general domain data without requiring manual annotation.</p>

    <p><a href="https://arxiv.org/abs/2211.02849"><img src="https://img.shields.io/badge/arxiv-2211.02849-white?style=flat&amp;logo=arxiv" alt="Static Badge" /></a></p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">VIS 2020</div><img src="/images/selected papers/Let‚Äôs gamble_ How a poor visualization can elicit risky behavior.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://ieeexplore.ieee.org/abstract/document/9331315">Let‚Äôs gamble: How a poor visualization can elicit risky behavior</a> <br />
M. Bancilhon*, <strong>Z. Liu</strong> *, and A. Ottley
<span style="background-color: #FFF9E6; font-style: italic; border-radius: 2px; font-size: 0.85em; color: #666;">
    (* equal contribution)
</span></p>

    <p>This study utilizes a large-scale gambling game to demonstrate that while icon arrays encourage economically rational decision-making, area-proportioned designs like circles and triangles significantly bias users towards risky behavior (gambling) even when it is not the optimal choice.</p>

    <p><a href="https://arxiv.org/abs/2010.14069"><img src="https://img.shields.io/badge/arxiv-2010.14069-white?style=flat&amp;logo=arxiv" alt="Static Badge" /></a></p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">Meta-radiology IF=18.26</div><img src="/images/selected papers/1-s2.0-S2950162823000176-gr2.jpg" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://www.sciencedirect.com/science/article/pii/S2950162823000176?__cf_chl_tk=9lCFejbbUQWyubIjxmzJWn1JjTdLchNofxxpxPlsS2E-1769171863-1.0.1.1-yNlkF9aYdU5LOs2iGJwRYU4o5ZzwoRl.VZ96eqviwJA">Summary of ChatGPT-related research and perspective towards the future of large language models</a> <br />
Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He, <strong>Z. Liu</strong>, Z. Wu, L. Zhao, D. Zhu, X. Li, N. Qiang, D. Shen, T. Liu, and B. Ge</p>

    <p>This paper presents a comprehensive survey of 194 ChatGPT-related studies, providing a detailed analysis of the model‚Äôs technical foundations (such as RLHF), its diverse applications across domains like medicine and education, and its ethical implications, while outlining future directions for large language model development.</p>

    <p><a href="https://arxiv.org/abs/2304.01852"><img src="https://img.shields.io/badge/arxiv-2304.01852-white?style=flat&amp;logo=arxiv" alt="Static Badge" /></a>
<a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=p8tAM0AAAAAJ&amp;citation_for_view=p8tAM0AAAAAJ:ULOm3_A8WrAC"><img src="https://img.shields.io/badge/Citations-1514-Blue?style=flat&amp;logo=Google%20Scholar" alt="Static Badge" /></a></p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">IEEE TBD IF=5.7</div><img src="/images/selected papers/Auggpt_Leveraging chatgpt for text data augmentation.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://ieeexplore.ieee.org/abstract/document/10858342/">AugGPT: Leveraging ChatGPT for text data augmentation</a> <br />
H. Dai*, <strong>Z. Liu</strong> *, W. Liao, X. Huang, Y. Cao, Z. Wu, L. Zhao, S. Xu, W. Liu, N. Liu, and T. Liu
<span style="background-color: #FFF9E6; font-style: italic; border-radius: 2px; font-size: 0.85em; color: #666;">
    (* equal contribution)
</span></p>

    <p>This paper introduces AugGPT, a data augmentation framework that utilizes ChatGPT to rephrase original training data into semantically consistent but stylistically diverse samples, significantly boosting model performance and robustness in few-shot text classification tasks.</p>

    <p><a href="https://arxiv.org/abs/2302.13007"><img src="https://img.shields.io/badge/arxiv-2302.13007-white?style=flat&amp;logo=arxiv" alt="Static Badge" /></a></p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">Meta-Radiology IF=18.26</div><img src="/images/selected papers/Radiology_GPT_ A Large Language Model for Radiology.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://www.sciencedirect.com/science/article/pii/S2950162825000219">Radiology-GPT: a large language model for radiology</a> <br />
<strong>Z. Liu</strong>, Y. Li, P. Shu, A. Zhong, H. Jiang, Y. Pan, L. Yang, C. Ju, Z. Wu, C. Ma, C. Chen, S. Kim, H. Dai, L. Zhao, L. Sun, D. Zhu, J. Liu, W. Liu, D. Shen, Q. Li, T. Liu, and X. Li</p>

    <p>This paper presents Radiology-GPT, a domain-specific large language model developed via instruction tuning on radiology reports, which achieves superior performance in diagnostic reasoning and report generation compared to general-purpose models while ensuring data privacy for clinical deployment.</p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">Nature Medicine IF=50.0</div><img src="/images/selected papers/A generalist vision_language foundation model for diverse biomedical tasks.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://www.nature.com/articles/s41591-024-03185-2">A generalist vision‚Äìlanguage foundation model for diverse biomedical tasks</a> <br />
C. Yan, J. Bi, Y. Luo, Y. Ma, <strong>Z. Liu</strong>, Z. Wu, L. Zhao, S. Xu, L. Wei, S. Huang, H. Wang, Y. Pan, B. Liao, Y. Huang, J. Xia, M. He, Z. Wang, Z. Lin, C. Slaughter, H. Zhu, Y. Zhang, Q. Qu, X. Zhang, G. Li, S. Ju, J. Huang, S. S. Zhang, D. Zhou, R. J. Fu, L. Sun, P. S. Yu, W. Liu, J. Gao, X. Li, D. Zhu, T. Liu, and D. Shen</p>

    <p>This study introduces BiomedGPT, a unified and open-source foundation model pre-trained on diverse multi-modal biomedical data (including 2D/3D images and text), which demonstrates that a single generalist model can effectively transfer knowledge across varying domains to perform a wide range of tasks such as image classification, captioning, and visual question answering.</p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">IEEE TPAMI IF=21.9</div><img src="/images/selected papers/Structure mapping generative adversarial network for multi_view information mapping pattern mining.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://www.sciencedirect.com/science/article/pii/S2950162825000219">Structure mapping generative adversarial network for multi-view information mapping pattern mining</a> \
X. A. Bi, Y. Huang, Z. Yang, K. Chen, Z. Xing, L. Xu, X. Li, <strong>Z. Liu</strong>, and T. Liu</p>

    <p>This paper proposes a Structure Mapping Generative Adversarial Network (SM-GAN), a framework that models the hierarchical interactions between different data views as a structural mapping process from micro- to macro-networks, effectively capturing common patterns to improve performance in multi-view learning tasks such as classification and evolution prediction.</p>

  </div>
</div>

<!-- ---

<div id="clustrmaps-widget" style="width:20%">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=292&t=n&d=-lU7AEiVU3EjLRsDUMPdzMI6Pk188Yruo8VxwvmHugY'></script>
</div> -->

          </section>
        </div>
      </article>
    </div>

    <script src="/assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/CounterDreamer/Zhengliang-Liu.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            // var totalCitation = data['citedby']
            // document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
