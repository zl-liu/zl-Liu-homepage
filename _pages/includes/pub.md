
# üìù Recent Publications [[more...]]({{ site.baseurl }}/publications/)


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AIED 2023</div><img src='{{ "/images/selected papers/Context Matters_ A Strategy to Pre_train.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Context matters: A strategy to pre-train language model for science education](https://link.springer.com/chapter/10.1007/978-3-031-36336-8_103) \\
**Z. Liu**, X. He, L. Liu, T. Liu, and X. Zhai

This study proposes a domain-specific pre-training strategy that significantly improves the automatic scoring of student science responses by continually training BERT models on specialized educational corpora (such as student answers and journal articles) to better capture the unique linguistic patterns of student scientific argumentation.

[![Static Badge](https://img.shields.io/badge/arxiv-2301.12031-white?style=flat&logo=arxiv)](https://arxiv.org/abs/2301.12031)

</div>
</div>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">MICCAI 2025</div><img src='{{ "/images/arxiv/2503.24368.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Adapting Vision Foundation Models for Real-time Ultrasound Image Segmentation](https://arxiv.org/pdf/2503.24368) \\
Xiaoran Zhang, Eric Z. Chen, **Lin Zhao**, Xiao Chen, Yikang Liu, Boris Maihe, James S Duncan, Terrence Chen, and Shanhui Sun

[![Static Badge](https://img.shields.io/badge/arxiv-2503.24368-white?style=flat&logo=arxiv)](https://arxiv.org/pdf/2503.24368)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TNNLS</div><img src='{{ "/images/arxiv/2408.08813.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Retrieval-Augmented Few-Shot Medical Image Segmentation with Foundation Models](https://arxiv.org/pdf/2408.08813) \\
**Lin Zhao**, Xiao Chen, Eric Z. Chen, Yikang Liu, Terrence Chen, and Shanhui Sun

[![Static Badge](https://img.shields.io/badge/arxiv-2408.08813-white?style=flat&logo=arxiv)](https://arxiv.org/pdf/2408.08813)

</div>
</div> -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MLMI 2023</div><img src='{{ "/images/selected papers/Tailoring Large Language Models to Radiology_ A Preliminary Approach to LLM Adaptation for a Highly Specialized Domain.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Tailoring large language models to radiology: A preliminary approach to llm adaptation for a highly specialized domain](https://link.springer.com/chapter/10.1007/978-3-031-45673-2_46) \\
**Z. Liu**, A. Zhong, Y. Li, L. Yang, C. Ju, Z. Wu, C. Ma, P. Shu, C. Chen, S. Kim, H. Dai, L. Zhao, D. Zhu, J. Liu, W. Liu, D. Shen, Q. Li, T. Liu, and X. Li

This preliminary study demonstrates the effectiveness of instruction tuning on radiological data to create a privacy-compliant, domain-specific large language model that outperforms general-purpose models (such as StableLM and LLaMA) in specialized tasks like radiological diagnosis and report generation.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCAI 2022</div><img src='{{ "/images/selected papers/AgriBERT_ Knowledge_Infused Agricultural Language Models for Matching Food and Nutrition.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AgriBERT: Knowledge-Infused Agricultural Language Models for Matching Food and Nutrition](https://www.researchgate.net/profile/Amulya-Yadav-2/publication/362052926_Forecasting_the_Number_of_Tenants_At-Risk-of-Formal_Eviction_A_Machine_Learning_Approach_to_Inform_Public_Policy/links/642eef0320f25554da139319/Forecasting-the-Number-of-Tenants-At-Risk-of-Formal-Eviction-A-Machine-Learning-Approach-to-Inform-Public-Policy.pdf) \\
S. Rezayi *, **Z. Liu** *, Z. Wu, C. Dhakal, B. Ge, C. Zhen, T. Liu, and S. Li
<span style="background-color: #FFF9E6; font-style: italic; border-radius: 2px; font-size: 0.85em; color: #666;">
    (* equal contribution)
</span>

This research presents AgriBERT, a specialized language model pre-trained on agricultural text and enhanced with knowledge infusion from food ontologies, designed to automate and significantly improve the accuracy of mapping unstructured food descriptions to standard nutritional databases.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BIBM 2023</div><img src='{{ "/images/selected papers/Coarse_to_fine Knowledge Graph Domain Adaptation based on.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Coarse-to-fine knowledge graph domain adaptation based on distantly-supervised iterative training](https://ieeexplore.ieee.org/abstract/document/10385649) \\
W. Liao*, **Z. Liu** *, Y. Zhang, X. Huang, F. Qi, S. Ding, H. Ren, Z. Wu, H. Dai, S. Li, L. Wu, N. Liu, Q. Li, T. Liu, X. Li, and H. Cai
<span style="background-color: #FFF9E6; font-style: italic; border-radius: 2px; font-size: 0.85em; color: #666;">
    (* equal contribution)
</span>

This paper proposes a coarse-to-fine domain adaptation framework that leverages distant supervision and an iterative training strategy to efficiently construct specialized knowledge graphs (such as for oncology) from general domain data without requiring manual annotation.

[![Static Badge](https://img.shields.io/badge/arxiv-2211.02849-white?style=flat&logo=arxiv)](https://arxiv.org/abs/2211.02849)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">VIS 2020</div><img src='{{ "/images/selected papers/Let's gamble_ How a poor visualization can elicit risky behavior.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Let's gamble: How a poor visualization can elicit risky behavior](https://ieeexplore.ieee.org/abstract/document/9331315) \\
M. Bancilhon*, **Z. Liu** *, and A. Ottley
<span style="background-color: #FFF9E6; font-style: italic; border-radius: 2px; font-size: 0.85em; color: #666;">
    (* equal contribution)
</span>

This study utilizes a large-scale gambling game to demonstrate that while icon arrays encourage economically rational decision-making, area-proportioned designs like circles and triangles significantly bias users towards risky behavior (gambling) even when it is not the optimal choice.

[![Static Badge](https://img.shields.io/badge/arxiv-2010.14069-white?style=flat&logo=arxiv)](https://arxiv.org/abs/2010.14069)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Meta-radiology IF=18.26</div><img src='{{ "/images/selected papers/1-s2.0-S2950162823000176-gr2.jpg" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Summary of ChatGPT-related research and perspective towards the future of large language models](https://www.sciencedirect.com/science/article/pii/S2950162823000176?__cf_chl_tk=9lCFejbbUQWyubIjxmzJWn1JjTdLchNofxxpxPlsS2E-1769171863-1.0.1.1-yNlkF9aYdU5LOs2iGJwRYU4o5ZzwoRl.VZ96eqviwJA) \\
Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He, **Z. Liu**, Z. Wu, L. Zhao, D. Zhu, X. Li, N. Qiang, D. Shen, T. Liu, and B. Ge

This paper presents a comprehensive survey of 194 ChatGPT-related studies, providing a detailed analysis of the model's technical foundations (such as RLHF), its diverse applications across domains like medicine and education, and its ethical implications, while outlining future directions for large language model development.

[![Static Badge](https://img.shields.io/badge/arxiv-2304.01852-white?style=flat&logo=arxiv)](https://arxiv.org/abs/2304.01852)
[![Static Badge](https://img.shields.io/badge/Citations-1514-Blue?style=flat&logo=Google%20Scholar)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=p8tAM0AAAAAJ&citation_for_view=p8tAM0AAAAAJ:ULOm3_A8WrAC)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TBD IF=5.7</div><img src='{{ "/images/selected papers/Auggpt_Leveraging chatgpt for text data augmentation.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AugGPT: Leveraging ChatGPT for text data augmentation](https://ieeexplore.ieee.org/abstract/document/10858342/) \\
H. Dai*, **Z. Liu** *, W. Liao, X. Huang, Y. Cao, Z. Wu, L. Zhao, S. Xu, W. Liu, N. Liu, and T. Liu
<span style="background-color: #FFF9E6; font-style: italic; border-radius: 2px; font-size: 0.85em; color: #666;">
    (* equal contribution)
</span>

This paper introduces AugGPT, a data augmentation framework that utilizes ChatGPT to rephrase original training data into semantically consistent but stylistically diverse samples, significantly boosting model performance and robustness in few-shot text classification tasks.

[![Static Badge](https://img.shields.io/badge/arxiv-2302.13007-white?style=flat&logo=arxiv)](https://arxiv.org/abs/2302.13007)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Meta-Radiology IF=18.26</div><img src='{{ "/images/selected papers/Radiology_GPT_ A Large Language Model for Radiology.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Radiology-GPT: a large language model for radiology](https://www.sciencedirect.com/science/article/pii/S2950162825000219) \\
**Z. Liu**, Y. Li, P. Shu, A. Zhong, H. Jiang, Y. Pan, L. Yang, C. Ju, Z. Wu, C. Ma, C. Chen, S. Kim, H. Dai, L. Zhao, L. Sun, D. Zhu, J. Liu, W. Liu, D. Shen, Q. Li, T. Liu, and X. Li

This paper presents Radiology-GPT, a domain-specific large language model developed via instruction tuning on radiology reports, which achieves superior performance in diagnostic reasoning and report generation compared to general-purpose models while ensuring data privacy for clinical deployment.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Nature Medicine IF=50.0</div><img src='{{ "/images/selected papers/A generalist vision_language foundation model for diverse biomedical tasks.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A generalist vision‚Äìlanguage foundation model for diverse biomedical tasks](https://www.nature.com/articles/s41591-024-03185-2) \\
C. Yan, J. Bi, Y. Luo, Y. Ma, **Z. Liu**, Z. Wu, L. Zhao, S. Xu, L. Wei, S. Huang, H. Wang, Y. Pan, B. Liao, Y. Huang, J. Xia, M. He, Z. Wang, Z. Lin, C. Slaughter, H. Zhu, Y. Zhang, Q. Qu, X. Zhang, G. Li, S. Ju, J. Huang, S. S. Zhang, D. Zhou, R. J. Fu, L. Sun, P. S. Yu, W. Liu, J. Gao, X. Li, D. Zhu, T. Liu, and D. Shen

This study introduces BiomedGPT, a unified and open-source foundation model pre-trained on diverse multi-modal biomedical data (including 2D/3D images and text), which demonstrates that a single generalist model can effectively transfer knowledge across varying domains to perform a wide range of tasks such as image classification, captioning, and visual question answering.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TPAMI IF=21.9</div><img src='{{ "/images/selected papers/Structure mapping generative adversarial network for multi_view information mapping pattern mining.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Structure mapping generative adversarial network for multi-view information mapping pattern mining](https://www.sciencedirect.com/science/article/pii/S2950162825000219) \
X. A. Bi, Y. Huang, Z. Yang, K. Chen, Z. Xing, L. Xu, X. Li, **Z. Liu**, and T. Liu

This paper proposes a Structure Mapping Generative Adversarial Network (SM-GAN), a framework that models the hierarchical interactions between different data views as a structural mapping process from micro- to macro-networks, effectively capturing common patterns to improve performance in multi-view learning tasks such as classification and evolution prediction.

</div>
</div>
